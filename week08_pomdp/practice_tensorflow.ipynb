{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Kung-Fu with advantage actor-critic\n",
    "\n",
    "In this notebook you'll build a deep reinforcement learning agent for Atari [Kung-Fu Master](https://gym.openai.com/envs/KungFuMaster-v0/) that uses a recurrent neural net.\n",
    "\n",
    "![https://upload.wikimedia.org/wikipedia/en/6/66/Kung_fu_master_mame.png](https://upload.wikimedia.org/wikipedia/en/6/66/Kung_fu_master_mame.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "if 'google.colab' in sys.modules:    \n",
    "    if not os.path.exists('.setup_complete'):\n",
    "        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/spring20/setup_colab.sh -O- | bash\n",
    "        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/spring20/week08_pomdp/atari_util.py\n",
    "        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/spring20/week08_pomdp/env_pool.py\n",
    "        !touch .setup_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are running on a server, launch xvfb to record game videos\n",
    "# Please make sure you have xvfb installed\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For starters, let's take a look at the game itself:\n",
    "\n",
    "* Image resized to 42x42 and converted to grayscale to run faster\n",
    "* Agent sees last 4 frames of game to account for object velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from atari_util import PreprocessAtari\n",
    "\n",
    "def make_env():\n",
    "    env = gym.make(\"KungFuMasterDeterministic-v0\")\n",
    "    env = PreprocessAtari(\n",
    "        env, height=42, width=42,\n",
    "        crop=lambda img: img[60:-30, 5:],\n",
    "        dim_order='tensorflow',\n",
    "        color=False, n_frames=4)\n",
    "    return env\n",
    "\n",
    "env = make_env()\n",
    "obs_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"Observation shape:\", obs_shape)\n",
    "print(\"Num actions:\", n_actions)\n",
    "print(\"Action names:\", env.env.env.get_action_meanings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = env.reset()\n",
    "for _ in range(100):\n",
    "    s, _, _, _ = env.step(env.action_space.sample())\n",
    "\n",
    "plt.title('Game image')\n",
    "plt.imshow(env.render('rgb_array'))\n",
    "plt.show()\n",
    "\n",
    "plt.title('Agent observation (4-frame buffer)')\n",
    "plt.imshow(s.transpose([0, 2, 1]).reshape([42,-1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POMDP setting\n",
    "\n",
    "The atari game we're working with is actually a POMDP: your agent needs to know timing at which enemies spawn and move, but cannot do so unless it has some memory.\n",
    "\n",
    "Let's design another agent that has a recurrent neural net memory to solve this. Here's a sketch.\n",
    "\n",
    "![img](img1_tf.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, LSTMCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRecurrentAgent:\n",
    "    def __init__(self, obs_shape, n_actions):\n",
    "        \"\"\"A simple actor-critic agent\"\"\"\n",
    "\n",
    "        # Let's define our computational graph\n",
    "        # Note: number of units/filters is arbitrary, you can change it at your will\n",
    "\n",
    "        # Our backbone is denoted by the part before LSTM\n",
    "        self.backbone = tf.keras.Sequential((\n",
    "            Conv2D(32, (3, 3), strides=(2, 2), activation='elu'),\n",
    "            Conv2D(32, (3, 3), strides=(2, 2), activation='elu'),\n",
    "            Conv2D(32, (3, 3), strides=(2, 2), activation='elu'),\n",
    "            Flatten(),\n",
    "            Dense(256, activation='relu'))\n",
    "        )\n",
    "        self.lstm_output_size = 256\n",
    "        self.lstm_cell = LSTMCell(self.lstm_output_size)\n",
    "\n",
    "        self.logits_head = Dense(n_actions)\n",
    "        self.state_value_head = Dense(1)\n",
    "\n",
    "    @property\n",
    "    def trainable_variables(self):\n",
    "        return self.backbone.trainable_variables + \\\n",
    "               self.lstm_cell.trainable_variables + \\\n",
    "               self.logits_head.trainable_variables + \\\n",
    "               self.state_value_head.trainable_variables\n",
    "\n",
    "    def __call__(self, prev_state, obs_t):\n",
    "        return self.forward(prev_state, obs_t)\n",
    "\n",
    "    def forward(self, prev_state, obs_t):\n",
    "        \"\"\"\n",
    "        Takes agent's previous hidden state and a new observation,\n",
    "        returns a new hidden state and whatever the agent needs to learn\n",
    "        \"\"\"\n",
    "\n",
    "        # Apply the whole neural net for one step here.\n",
    "        # See docs on self.lstm_cell(...).\n",
    "        # The recurrent cell should take otuput of the backbone as input.\n",
    "        <YOUR CODE>\n",
    "\n",
    "        new_state = <YOUR CODE>\n",
    "        logits = <YOUR CODE>\n",
    "        state_value = <YOUR CODE, please sqeeze the last dimention\n",
    "                       to more convinient shape (n, ) instead of (n, 1)>\n",
    "        return new_state, (logits, state_value)\n",
    "\n",
    "    def step(self, prev_state, obs_t):\n",
    "        \"\"\"Same as forward except it takes as input and returns numpy arrays\n",
    "           (or maybe lists of numpy arrays as input, they're allowed as well)\"\"\"\n",
    "        prev_state = [tf.convert_to_tensor(prev_state[0], dtype='float32'),\n",
    "                      tf.convert_to_tensor(prev_state[1], dtype='float32')]\n",
    "        obs_t = tf.convert_to_tensor(obs_t, dtype='float32')\n",
    "        new_state, outputs = self.forward(prev_state, obs_t)\n",
    "        new_state = (new_state[0].numpy(), new_state[1].numpy())\n",
    "        outputs = (outputs[0].numpy(), outputs[1].numpy())\n",
    "        return new_state, outputs\n",
    "\n",
    "    def get_initial_state(self, batch_size):\n",
    "        \"\"\"Return a list of agent memory states at game start.\n",
    "           Each state is a tf tensor of shape [batch_size, ...]\"\"\"\n",
    "        # feedforward agent has no state\n",
    "        return [tf.zeros([batch_size, self.lstm_output_size], dtype='float32'),\n",
    "                tf.zeros([batch_size, self.lstm_output_size], dtype='float32')]\n",
    "\n",
    "    def sample_actions(self, agent_outputs):\n",
    "        \"\"\"pick actions given numeric agent outputs (numpy arrays)\"\"\"\n",
    "        logits, state_values = agent_outputs\n",
    "        policy = tf.nn.softmax(logits, axis=-1).numpy()\n",
    "        return [np.random.choice(len(p), p=p) for p in policy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_parallel_games = 5\n",
    "gamma = 0.99\n",
    "\n",
    "agent = SimpleRecurrentAgent(obs_shape, n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()[np.newaxis, :].astype('float32')\n",
    "_, (logits, value) = agent.step(agent.get_initial_state(1), obs)\n",
    "print(\"action logits:\\n\", logits)\n",
    "print(\"state values:\\n\", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's play!\n",
    "Let's build a function that measures agent's average reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, env, n_games=1):\n",
    "    \"\"\"Plays an a game from start till done, returns per-game rewards \"\"\"\n",
    "\n",
    "    game_rewards = []\n",
    "    for _ in range(n_games):\n",
    "        # initial observation and memory\n",
    "        observation = env.reset()\n",
    "        prev_memories = agent.get_initial_state(1)\n",
    "\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            observations = observation[np.newaxis, :].astype('float32')\n",
    "            new_memories, readouts = agent.forward(\n",
    "                prev_memories, observations)\n",
    "            action = agent.sample_actions(readouts)\n",
    "\n",
    "            observation, reward, done, info = env.step(action[0])\n",
    "\n",
    "            total_reward += reward\n",
    "            prev_memories = new_memories\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        game_rewards.append(total_reward)\n",
    "    return game_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import gym.wrappers\n",
    "\n",
    "with gym.wrappers.Monitor(make_env(), directory=\"videos\", force=True) as env_monitor:\n",
    "    rewards = evaluate(agent, env_monitor, n_games=3)\n",
    "\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show video. This may not work in some setups. If it doesn't\n",
    "# work for you, you can download the videos and view them locally.\n",
    "\n",
    "from pathlib import Path\n",
    "from IPython.display import HTML\n",
    "\n",
    "video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(video_names[-1]))  # You can also try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on parallel games\n",
    "\n",
    "We introduce a class called EnvPool - it's a tool that handles multiple environments for you. Here's how it works:\n",
    "![img](https://s7.postimg.cc/4y36s2b2z/env_pool.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env_pool import EnvPool\n",
    "pool = EnvPool(agent, make_env, n_parallel_games)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gonna train our agent on a thing called __rollouts:__\n",
    "![img](img3.jpg)\n",
    "\n",
    "A rollout is just a sequence of T observations, actions and rewards that agent took consequently.\n",
    "* First __s0__ is not necessarily initial state for the environment\n",
    "* Final state is not necessarily terminal\n",
    "* We sample several parallel rollouts for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# for each of n_parallel_games, take 10 steps\n",
    "rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Actions shape:\", rollout_actions.shape)\n",
    "print(\"Rewards shape:\", rollout_rewards.shape)\n",
    "print(\"Mask shape:\", rollout_mask.shape)\n",
    "print(\"Observations shape: \", rollout_obs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-critic objective\n",
    "\n",
    "Here we define a loss function that uses rollout above to train advantage actor-critic agent.\n",
    "\n",
    "\n",
    "Our loss consists of three components:\n",
    "\n",
    "* __The policy \"loss\"__\n",
    " $$ \\hat J = {1 \\over T} \\cdot \\sum_t { \\log \\pi(a_t | s_t) } \\cdot A_{const}(s,a) $$\n",
    "  * This function has no meaning in and of itself, but it was built such that\n",
    "  * $ \\nabla \\hat J = {1 \\over N} \\cdot \\sum_t { \\nabla \\log \\pi(a_t | s_t) } \\cdot A(s,a) \\approx \\nabla E_{s, a \\sim \\pi} R(s,a) $\n",
    "  * Therefore if we __maximize__ J_hat with gradient descent we will maximize expected reward\n",
    "  \n",
    "  \n",
    "* __The value \"loss\"__\n",
    "  $$ L_{td} = {1 \\over T} \\cdot \\sum_t { [r + \\gamma \\cdot V_{const}(s_{t+1}) - V(s_t)] ^ 2 }$$\n",
    "  * Ye Olde TD_loss from q-learning and alike\n",
    "  * If we minimize this loss, V(s) will converge to $V_\\pi(s) = E_{a \\sim \\pi(a | s)} R(s,a) $\n",
    "\n",
    "\n",
    "* __Entropy Regularizer__\n",
    "  $$ H = - {1 \\over T} \\sum_t \\sum_a {\\pi(a|s_t) \\cdot \\log \\pi (a|s_t)}$$\n",
    "  * If we __maximize__ entropy we discourage agent from predicting zero probability to actions\n",
    "  prematurely (a.k.a. exploration)\n",
    "  \n",
    "  \n",
    "So we optimize a linear combination of $L_{td}$ $- \\hat J$, $-H$\n",
    "  \n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "__One more thing:__ since we train on T-step rollouts, we can use N-step formula for advantage for free:\n",
    "  * At the last step, $A(s_t,a_t) = r(s_t, a_t) + \\gamma \\cdot V(s_{t+1}) - V(s) $\n",
    "  * One step earlier, $A(s_t,a_t) = r(s_t, a_t) + \\gamma \\cdot r(s_{t+1}, a_{t+1}) + \\gamma ^ 2 \\cdot V(s_{t+2}) - V(s) $\n",
    "  * Et cetera, et cetera. This way agent starts training much faster since it's estimate of A(s,a) depends less on his (imperfect) value function and more on actual rewards. There's also a [nice generalization](https://arxiv.org/abs/1506.02438) of this.\n",
    "\n",
    "\n",
    "__Note:__ it's also a good idea to scale rollout_len up to learn longer sequences. You may wish set it to >=20 or to start at 10 and then scale up as time passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_log_policy_for_actions(log_policy, actions):\n",
    "    # This code selects the log-probabilities (log pi(a_i|s_i))\n",
    "    # for those actions that were actually played.\n",
    "    actions_one_hot = tf.one_hot(actions, n_actions)\n",
    "    log_policy_for_actions = tf.reduce_sum(log_policy * actions_one_hot, axis=-1)\n",
    "    return log_policy_for_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_rollout(states, actions, rewards, is_not_done, prev_memory_states, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
    "    Updates agent's weights by following the policy gradient above.\n",
    "    Please use Adam optimizer with default parameters.\n",
    "    \"\"\"\n",
    "    # shape: [batch_size, time, c, h, w]\n",
    "    states = tf.convert_to_tensor(states, dtype='float32')\n",
    "    actions = tf.convert_to_tensor(actions, dtype='int32')  # shape: [batch_size, time]\n",
    "    rewards = tf.convert_to_tensor(rewards, dtype='float32')  # shape: [batch_size, time]\n",
    "    is_not_done = tf.convert_to_tensor(is_not_done, dtype='float32')  # shape: [batch_size, time]\n",
    "    rollout_length = rewards.shape[1] - 1\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # We want to stop gradient here to prevent it from travelling a long distance \n",
    "        memory = [tf.stop_gradient(mem_state) for mem_state in prev_memory_states]\n",
    "\n",
    "        logits = []  # append logit sequence here\n",
    "        state_values = []  # append state values here\n",
    "        for t in range(rewards.shape[1]):\n",
    "            obs_t = states[:, t]\n",
    "\n",
    "            # use agent to comute logits_t and state values_t.\n",
    "            # append them to logits and state_values array\n",
    "\n",
    "            memory, (logits_t, values_t) = <YOUR CODE>\n",
    "\n",
    "            logits.append(logits_t)\n",
    "            state_values.append(values_t)\n",
    "\n",
    "        logits = tf.stack(logits, axis=1)\n",
    "        state_values = tf.stack(state_values, axis=1)\n",
    "        policy = tf.nn.softmax(logits, axis=2)\n",
    "        log_policy = tf.nn.log_softmax(logits, axis=2)\n",
    "\n",
    "        # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
    "        log_policy_for_actions = select_log_policy_for_actions(log_policy, actions)\n",
    "\n",
    "        # Now let's compute two loss components:\n",
    "        # 1) Policy gradient objective.\n",
    "        # Notes: Please don't forget to call stop_gradient on advantage term. Also please use mean, not sum.\n",
    "        # it's okay to use loops if you want\n",
    "        J_hat = 0  # policy objective as in the formula for J_hat\n",
    "\n",
    "        # 2) Temporal difference MSE for state values\n",
    "        # Notes: Please don't forget to call on V(s') term. Also please use mean, not sum.\n",
    "        # it's okay to use loops if you want\n",
    "        value_loss = 0\n",
    "\n",
    "        cumulative_returns = tf.stop_gradient(state_values[:, -1]) * is_not_done[:, -1]\n",
    "\n",
    "        for t in reversed(range(rollout_length)):\n",
    "            r_t = rewards[:, t]                                # current rewards\n",
    "            # current state values\n",
    "            V_t = state_values[:, t]\n",
    "            V_next = tf.stop_gradient(state_values[:, t + 1])  # next state values\n",
    "            # log-probability of a_t in s_t\n",
    "            logpi_a_s_t = log_policy_for_actions[:, t]\n",
    "\n",
    "            # update G_t = r_t + gamma * G_{t+1} as we did in week6 reinforce\n",
    "            cumulative_returns = G_t = r_t + gamma * cumulative_returns * is_not_done[:, t]\n",
    "\n",
    "            # Compute temporal difference error (MSE for V(s))\n",
    "            value_loss += <YOUR CODE>\n",
    "\n",
    "            # compute advantage A(s_t, a_t) using cumulative returns and V(s_t) as baseline\n",
    "            advantage = <YOUR CODE>\n",
    "            advantage = tf.stop_gradient(advantage)\n",
    "\n",
    "            # compute policy pseudo-loss aka -J_hat.\n",
    "            J_hat += <YOUR CODE>\n",
    "\n",
    "        # regularize with entropy\n",
    "        entropy_reg = <YOUR CODE: compute entropy regularizer, please mind the sign>\n",
    "\n",
    "        # add-up three loss components and average over time\n",
    "        loss = -J_hat / rollout_length +\\\n",
    "            value_loss / rollout_length +\\\n",
    "               -0.01 * entropy_reg\n",
    "\n",
    "        # Gradient descent step\n",
    "        grads = tape.gradient(loss, agent.trainable_variables)\n",
    "    <YOUR CODE: apply gradients>\n",
    "\n",
    "    return loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = <YOUR CODE: choose your favorite optimizer>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's test it\n",
    "memory = list(pool.prev_memory_states)\n",
    "rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(10)\n",
    "\n",
    "train_on_rollout(rollout_obs, rollout_actions,\n",
    "                 rollout_rewards, rollout_mask, memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train \n",
    "\n",
    "just run train step and see if agent learns any better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import trange\n",
    "from pandas import DataFrame\n",
    "moving_average = lambda x, **kw: DataFrame(\n",
    "    {'x': np.asarray(x)}).x.ewm(**kw).mean().values\n",
    "\n",
    "rewards_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in trange(15000):\n",
    "    memory = list(pool.prev_memory_states)\n",
    "    rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(10)\n",
    "    train_on_rollout(rollout_obs, rollout_actions,\n",
    "                     rollout_rewards, rollout_mask, memory)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        rewards_history.append(np.mean(evaluate(agent, env, n_games=1)))\n",
    "        clear_output(True)\n",
    "        plt.plot(rewards_history, label='rewards', linewidth=3)\n",
    "        plt.plot(moving_average(np.array(rewards_history),\n",
    "                                span=10), label='rewards ewma@10', linewidth=3)\n",
    "        plt.legend(fontsize=13)\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        if rewards_history[-1] >= 10000:\n",
    "            print(\"Your agent has just passed the minimum homework threshold\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relax and grab some refreshments while your agent is locked in an infinite loop of violence and death.\n",
    "\n",
    "__How to interpret plots:__\n",
    "\n",
    "The session reward is the easy thing: it should in general go up over time, but it's okay if it fluctuates ~~like crazy~~. It's also OK if it reward doesn't increase substantially before some 10k initial steps. However, if reward reaches zero and doesn't seem to get up over 2-3 evaluations, there's something wrong happening.\n",
    "\n",
    "\n",
    "Since we use a policy-based method, we also keep track of __policy entropy__ - the same one you used as a regularizer. The only important thing about it is that your entropy shouldn't drop too low (`< 0.1`) before your agent gets the yellow belt. Or at least it can drop there, but _it shouldn't stay there for long_.\n",
    "\n",
    "If it does, the culprit is likely:\n",
    "* Some bug in entropy computation. Remember that it is $ - \\sum p(a_i) \\cdot log p(a_i) $\n",
    "* Your agent architecture converges too fast. Increase entropy coefficient in actor loss. \n",
    "* Gradient explosion - just clip gradients and maybe use a smaller network\n",
    "* Us. Or TensorFlow developers. Or aliens. Or lizardfolk. Contact us on forums before it's too late!\n",
    "\n",
    "If you're debugging, just run `logits, values = agent.step(batch_states)` and manually look into logits and values. This will reveal the problem 9 times out of 10: you'll likely see some NaNs or insanely large numbers or zeros. Try to catch the moment when this happens for the first time and investigate from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Final\" evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym.wrappers\n",
    "\n",
    "with gym.wrappers.Monitor(make_env(), directory=\"videos\", force=True) as env_monitor:\n",
    "    final_rewards = evaluate(agent, env_monitor, n_games=20)\n",
    "\n",
    "print(\"Final mean reward\", np.mean(final_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show video. This may not work in some setups. If it doesn't\n",
    "# work for you, you can download the videos and view them locally.\n",
    "\n",
    "from pathlib import Path\n",
    "from IPython.display import HTML\n",
    "\n",
    "video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(video_names[-1]))  # You can also try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
